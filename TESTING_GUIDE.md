# Nanobot å•å…ƒæµ‹è¯•ä¸è°ƒè¯•æŒ‡å—

æœ¬æ–‡æ¡£ä»¥ `TestIsOllama` ä¸ºä¾‹ï¼Œè¯¦ç»†ä»‹ç»å¦‚ä½•ç¼–å†™ã€è¿è¡Œå’Œè°ƒè¯•å•å…ƒæµ‹è¯•ã€‚

---

## ğŸ“‹ ç›®å½•

1. [æµ‹è¯•æ¡†æ¶æ¦‚è¿°](#æµ‹è¯•æ¡†æ¶æ¦‚è¿°)
2. [æµ‹è¯•æ–‡ä»¶ç»“æ„](#æµ‹è¯•æ–‡ä»¶ç»“æ„)
3. [ç¼–å†™æµ‹è¯•ç”¨ä¾‹](#ç¼–å†™æµ‹è¯•ç”¨ä¾‹)
4. [è¿è¡Œæµ‹è¯•](#è¿è¡Œæµ‹è¯•)
5. [è°ƒè¯•æŠ€å·§](#è°ƒè¯•æŠ€å·§)
6. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
7. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## æµ‹è¯•æ¡†æ¶æ¦‚è¿°

### æŠ€æœ¯æ ˆ

| ç»„ä»¶ | ç”¨é€” | ç‰ˆæœ¬ |
|-----|------|------|
| **pytest** | æµ‹è¯•æ¡†æ¶æ ¸å¿ƒ | >=7.0.0 |
| **pytest-asyncio** | å¼‚æ­¥æµ‹è¯•æ”¯æŒ | >=0.21.0 |

### é…ç½®ä½ç½®

æµ‹è¯•é…ç½®åœ¨ `pyproject.toml` ä¸­ï¼š

```toml
[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0",
]

[tool.pytest.ini_options]
asyncio_mode = "auto"
testpaths = ["tests"]
```

---

## æµ‹è¯•æ–‡ä»¶ç»“æ„

### ç›®å½•å¸ƒå±€

```
nanobot/
â”œâ”€â”€ tests/                          # æµ‹è¯•ç›®å½•
â”‚   â”œâ”€â”€ __init__.py                 # ç©ºæ–‡ä»¶ï¼Œä½¿ tests æˆä¸ºåŒ…
â”‚   â”œâ”€â”€ test_litellm_provider.py    # LiteLLMProvider æµ‹è¯•
â”‚   â”œâ”€â”€ test_tool_validation.py     # å·¥å…·éªŒè¯æµ‹è¯•
â”‚   â””â”€â”€ ...                         # å…¶ä»–æµ‹è¯•æ–‡ä»¶
â”œâ”€â”€ nanobot/                        # æºä»£ç 
â”‚   â””â”€â”€ providers/
â”‚       â””â”€â”€ litellm_provider.py     # è¢«æµ‹ä»£ç 
â””â”€â”€ pyproject.toml                  # æµ‹è¯•é…ç½®
```

### å‘½åè§„èŒƒ

| é¡¹ç›® | å‘½åè§„åˆ™ | ç¤ºä¾‹ |
|-----|---------|------|
| æµ‹è¯•æ–‡ä»¶ | `test_*.py` | `test_litellm_provider.py` |
| æµ‹è¯•ç±» | `Test*` | `TestIsOllama` |
| æµ‹è¯•æ–¹æ³• | `test_*` | `test_is_ollama_true_with_ollama_in_model` |

---

## ç¼–å†™æµ‹è¯•ç”¨ä¾‹

### å®Œæ•´ç¤ºä¾‹ï¼šTestIsOllama

```python
"""Tests for LiteLLMProvider."""

# æ¨¡å—ä½œç”¨ï¼šLiteLLMProvider çš„å•å…ƒæµ‹è¯•ï¼ŒéªŒè¯ Ollama æ£€æµ‹é€»è¾‘
# è®¾è®¡ç›®çš„ï¼šç¡®ä¿ is_ollama å±æ€§åœ¨å„ç§åœºæ™¯ä¸‹æ­£ç¡®è¯†åˆ«æœ¬åœ° Ollama éƒ¨ç½²
# å¥½å¤„ï¼šé˜²æ­¢å›å½’é”™è¯¯ï¼Œä¿éšœæœ¬åœ° LLM é›†æˆçš„å¯é æ€§
import pytest
from nanobot.providers.litellm_provider import LiteLLMProvider


# ä½œç”¨ï¼šæµ‹è¯• is_ollama å±æ€§çš„å„ç§åœºæ™¯æ£€æµ‹
# è®¾è®¡ç›®çš„ï¼šè¦†ç›–æ­£å¸¸æƒ…å†µå’Œè¾¹ç•Œæƒ…å†µï¼Œç¡®ä¿æ£€æµ‹é€»è¾‘å¥å£®
# å¥½å¤„ï¼šå…¨é¢éªŒè¯ Ollama æ£€æµ‹ï¼Œé˜²æ­¢è¯¯åˆ¤æˆ–æ¼åˆ¤
class TestIsOllama:
    """Test is_ollama property detection."""

    # ä½œç”¨ï¼šæµ‹è¯•æ¨¡å‹ååŒ…å« 'ollama' æ—¶æ­£ç¡®è¯†åˆ«ä¸º Ollama
    # è®¾è®¡ç›®çš„ï¼šéªŒè¯æœ€åŸºæœ¬çš„æ­£å‘æ£€æµ‹åœºæ™¯
    # å¥½å¤„ï¼šç¡®ä¿æ ‡å‡† Ollama æ¨¡å‹æ ¼å¼è¢«æ­£ç¡®è¯†åˆ«
    def test_is_ollama_true_with_ollama_in_model(self):
        """Test is_ollama is True when model contains 'ollama'."""
        provider = LiteLLMProvider(
            api_key="dummy",
            api_base="http://localhost:11434",
            default_model="ollama/llama3.2"
        )
        assert provider.is_ollama is True
 
    
    # ä½œç”¨ï¼šæµ‹è¯•ä½¿ç”¨ 127.0.0.1 åœ°å€çš„å…¸å‹ Ollama è®¾ç½®
    # è®¾è®¡ç›®çš„ï¼šéªŒè¯æœ¬åœ°åœ°å€é…ç½®çš„æ­£ç¡®è¯†åˆ«
    # å¥½å¤„ï¼šè¦†ç›–å¸¸è§çš„æœ¬åœ°éƒ¨ç½²åœºæ™¯ï¼ŒåŒ…æ‹¬å¸¦ /v1 è·¯å¾„çš„ç«¯ç‚¹
    def test_is_ollama_with_localhost_api_base(self):
        """Test is_ollama with typical Ollama localhost setup."""
        provider = LiteLLMProvider(
            api_key="dummy",
            api_base="http://127.0.0.1:11434/v1",
            default_model="ollama/mistral"
        )
        assert provider.is_ollama is True

    # ä½œç”¨ï¼šæ‰¹é‡æµ‹è¯•å¤šç§ Ollama æ¨¡å‹åç§°çš„è¯†åˆ«
    # è®¾è®¡ç›®çš„ï¼šå‚æ•°åŒ–æµ‹è¯•ï¼Œè¦†ç›–å¤šç§ Ollama æ¨¡å‹æ ¼å¼
    # å¥½å¤„ï¼šä¸€æ¬¡æ€§éªŒè¯å¤šç§æ¨¡å‹ï¼Œæé«˜æµ‹è¯•æ•ˆç‡å’Œè¦†ç›–ç‡
    def test_is_ollama_with_different_ollama_models(self):
        """Test is_ollama with various Ollama model names."""
        # æµ‹è¯•ç”¨ä¾‹ï¼š(æ¨¡å‹åç§°, é¢„æœŸç»“æœ)
        test_cases = [
            ("ollama/llama3.2", True),
            ("ollama/mistral", True),
            ("ollama/codellama", True),
            ("ollama/gemma:2b", True),
            ("ollama/phi3", True),
            ("claude-3-opus", False),
            ("gpt-4", False),
            ("gemini-pro", False),
        ]
        
        for model, expected in test_cases:
            provider = LiteLLMProvider(
                api_key="dummy",
                api_base="http://localhost:11434",
                default_model=model
            )
            assert provider.is_ollama == expected, f"Failed for model: {model}"

 
```

### æµ‹è¯•ç»“æ„è§£æ

#### 1. ä¸‰å±‚æ³¨é‡Šä½“ç³»

```python
# ç¬¬ä¸€å±‚ï¼šæ¨¡å—/ç±»çº§åˆ« - è¯´æ˜æ•´ä½“åŠŸèƒ½
# æ¨¡å—ä½œç”¨ï¼š...
# è®¾è®¡ç›®çš„ï¼š...
# å¥½å¤„ï¼š...

class TestXxx:
    # ç¬¬äºŒå±‚ï¼šç±»çº§åˆ« - è¯´æ˜æµ‹è¯•èŒƒå›´
    # ä½œç”¨ï¼š...
    # è®¾è®¡ç›®çš„ï¼š...
    # å¥½å¤„ï¼š...
    
    def test_xxx(self):
        # ç¬¬ä¸‰å±‚ï¼šæ–¹æ³•çº§åˆ« - è¯´æ˜å…·ä½“æµ‹è¯•ç‚¹
        # ä½œç”¨ï¼š...
        # è®¾è®¡ç›®çš„ï¼š...
        # å¥½å¤„ï¼š...
```

#### 2. æµ‹è¯•æ–¹æ³•ç»“æ„

```python
def test_xxx(self):
    """ç®€çŸ­çš„æµ‹è¯•æè¿°ï¼ˆæ˜¾ç¤ºåœ¨æµ‹è¯•æŠ¥å‘Šä¸­ï¼‰ã€‚"""
    # 1. å‡†å¤‡ï¼ˆArrangeï¼‰
    provider = LiteLLMProvider(...)
    
    # 2. æ‰§è¡Œï¼ˆActï¼‰
    result = provider.is_ollama
    
    # 3. æ–­è¨€ï¼ˆAssertï¼‰
    assert result is True
```

---

## è¿è¡Œæµ‹è¯•

### ç¯å¢ƒå‡†å¤‡

```bash
# 1. è¿›å…¥é¡¹ç›®ç›®å½•
cd e:/myAiProject/nanobot

# 2. å®‰è£…ä¾èµ–ï¼ˆåŒ…å«æµ‹è¯•ä¾èµ–ï¼‰
pip install -e ".[dev]"

# 3. éªŒè¯å®‰è£…
python -m pytest --version
```

### è¿è¡Œå‘½ä»¤è¯¦è§£

#### 1. è¿è¡Œæ•´ä¸ªæµ‹è¯•ç±»

```bash
# è¯­æ³•ï¼špytest æ–‡ä»¶è·¯å¾„::ç±»å
python -m pytest tests/test_litellm_provider.py::TestIsOllama -v
```

**è¾“å‡ºç¤ºä¾‹ï¼š**
```
tests/test_litellm_provider.py::TestIsOllama::test_is_ollama_true_with_ollama_in_model PASSED [ 11%]
tests/test_litellm_provider.py::TestIsOllama::test_is_ollama_true_with_uppercase PASSED [ 22%]
...
============================== 9 passed in 0.15s ===============================
```

#### 2. è¿è¡Œå•ä¸ªæµ‹è¯•æ–¹æ³•

```bash
# è¯­æ³•ï¼špytest æ–‡ä»¶è·¯å¾„::ç±»å::æ–¹æ³•å
python -m pytest tests/test_litellm_provider.py::TestIsOllama::test_is_ollama_true_with_ollama_in_model -v
```

#### 3. è¿è¡Œæ•´ä¸ªæµ‹è¯•æ–‡ä»¶

```bash
python -m pytest tests/test_litellm_provider.py -v
```

#### 4. è¿è¡Œæ‰€æœ‰æµ‹è¯•

```bash
python -m pytest -v
# æˆ–
python -m pytest tests/ -v
```

### å¸¸ç”¨å‚æ•°

| å‚æ•° | ä½œç”¨ | ç¤ºä¾‹ |
|-----|------|------|
| `-v` | è¯¦ç»†è¾“å‡º | `pytest -v` |
| `-s` | æ˜¾ç¤º print è¾“å‡º | `pytest -s` |
| `-x` | é¦–æ¬¡å¤±è´¥å³åœæ­¢ | `pytest -x` |
| `-k` | æŒ‰åç§°è¿‡æ»¤ | `pytest -k "ollama"` |
| `--tb=short` | ç®€çŸ­é”™è¯¯ä¿¡æ¯ | `pytest --tb=short` |
| `--tb=long` | å®Œæ•´é”™è¯¯ä¿¡æ¯ | `pytest --tb=long` |
| `-q` | å®‰é™æ¨¡å¼ | `pytest -q` |
| `--lf` | åªè¿è¡Œä¸Šæ¬¡å¤±è´¥çš„ | `pytest --lf` |
| `--ff` | å…ˆè¿è¡Œä¸Šæ¬¡å¤±è´¥çš„ | `pytest --ff` |

---

## è°ƒè¯•æŠ€å·§

### æ–¹æ³•ä¸€ï¼šä½¿ç”¨ print è¾“å‡º

```python
def test_is_ollama_debug(self):
    """Debug version with print statements."""
    provider = LiteLLMProvider(
        api_key="dummy",
        api_base="http://localhost:11434",
        default_model="ollama/llama3.2"
    )
    
    # æ·»åŠ è°ƒè¯•è¾“å‡º
    print(f"\napi_base: {provider.api_base}")
    print(f"default_model: {provider.default_model}")
    print(f"is_ollama: {provider.is_ollama}")
    
    assert provider.is_ollama is True
```

**è¿è¡Œï¼ˆéœ€è¦ `-s` å‚æ•°ï¼‰ï¼š**
```bash
python -m pytest tests/test_litellm_provider.py::TestIsOllama::test_is_ollama_debug -v -s
```

### æ–¹æ³•äºŒï¼šä½¿ç”¨ pytest çš„ pdb è°ƒè¯•

```bash
# å¤±è´¥æ—¶è‡ªåŠ¨è¿›å…¥ pdb
python -m pytest tests/test_litellm_provider.py::TestIsOllama -v --pdb

# åœ¨ä»£ç ä¸­è®¾ç½®æ–­ç‚¹
import pytest

def test_xxx(self):
    ...
    pytest.set_trace()  # æ–­ç‚¹
    ...
```

### æ–¹æ³•ä¸‰ï¼šVS Code è°ƒè¯•é…ç½®

åˆ›å»º `.vscode/launch.json`ï¼š

```json
{
  "version": "0.2.0",
  "configurations": [
    {
      "name": "Python: Debug Tests",
      "type": "debugpy",
      "request": "launch",
      "module": "pytest",
      "args": [
        "tests/test_litellm_provider.py::TestIsOllama",
        "-v"
      ],
      "console": "integratedTerminal",
      "justMyCode": false
    },
    {
      "name": "Python: Debug Current Test",
      "type": "debugpy",
      "request": "launch",
      "module": "pytest",
      "args": [
        "${relativeFile}::${selectedText}",
        "-v"
      ],
      "console": "integratedTerminal"
    }
  ]
}
```

**ä½¿ç”¨æ­¥éª¤ï¼š**
1. åœ¨æµ‹è¯•ä»£ç ä¸­è®¾ç½®æ–­ç‚¹ï¼ˆç‚¹å‡»è¡Œå·å·¦ä¾§ï¼‰
2. é€‰æ‹©è°ƒè¯•é…ç½® "Python: Debug Tests"
3. æŒ‰ F5 å¯åŠ¨è°ƒè¯•
4. ä½¿ç”¨ F10ï¼ˆå•æ­¥è·³è¿‡ï¼‰ã€F11ï¼ˆå•æ­¥è¿›å…¥ï¼‰

### æ–¹æ³•å››ï¼šç›´æ¥è¿è¡Œ Python ä»£ç 

åˆ›å»ºä¸´æ—¶è°ƒè¯•è„šæœ¬ `debug_test.py`ï¼š

```python
"""ä¸´æ—¶è°ƒè¯•è„šæœ¬ã€‚"""
from nanobot.providers.litellm_provider import LiteLLMProvider

# æµ‹è¯•åœºæ™¯ 1ï¼šæ­£å¸¸æƒ…å†µ
provider1 = LiteLLMProvider(
    api_key="dummy",
    api_base="http://localhost:11434",
    default_model="ollama/llama3.2"
)
print(f"Test 1 - Expected True: {provider1.is_ollama}")

# æµ‹è¯•åœºæ™¯ 2ï¼šæ—  api_base
provider2 = LiteLLMProvider(
    api_key="dummy",
    api_base=None,
    default_model="ollama/llama3.2"
)
print(f"Test 2 - Expected False: {provider2.is_ollama}")

# æµ‹è¯•åœºæ™¯ 3ï¼šGPT æ¨¡å‹
provider3 = LiteLLMProvider(
    api_key="dummy",
    api_base="http://localhost:11434",
    default_model="gpt-4"
)
print(f"Test 3 - Expected False: {provider3.is_ollama}")
```

è¿è¡Œï¼š
```bash
python debug_test.py
```

---

## å¸¸è§é—®é¢˜

### é—®é¢˜ 1ï¼šModuleNotFoundError

**ç°è±¡ï¼š**
```
ModuleNotFoundError: No module named 'nanobot'
```

**è§£å†³ï¼š**
```bash
# ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•ï¼Œå¹¶å®‰è£…é¡¹ç›®
pip install -e .

# æˆ–è®¾ç½® PYTHONPATH
export PYTHONPATH=$PYTHONPATH:e:/myAiProject/nanobot
```

### é—®é¢˜ 2ï¼šImportError: No module named 'litellm'

**ç°è±¡ï¼š**
```
ModuleNotFoundError: No module named 'litellm'
```

**è§£å†³ï¼š**
```bash
# å®‰è£…é¡¹ç›®åŠå…¶ä¾èµ–
pip install -e ".[dev]"

# æˆ–å•ç‹¬å®‰è£… litellm
pip install litellm
```

### é—®é¢˜ 3ï¼šæµ‹è¯•æœªè¢«æ”¶é›†

**ç°è±¡ï¼š**
```
collected 0 items
```

**åŸå› ä¸è§£å†³ï¼š**
1. **æ–‡ä»¶åä¸è§„èŒƒ** â†’ å¿…é¡»ä»¥ `test_` å¼€å¤´
2. **ç±»åä¸è§„èŒƒ** â†’ å¿…é¡»ä»¥ `Test` å¼€å¤´
3. **æ–¹æ³•åä¸è§„èŒƒ** â†’ å¿…é¡»ä»¥ `test_` å¼€å¤´
4. **ç¼ºå°‘ `__init__.py`** â†’ åœ¨ `tests/` ç›®å½•æ·»åŠ ç©ºæ–‡ä»¶

### é—®é¢˜ 4ï¼šæ–­è¨€å¤±è´¥ä½†ä¸çŸ¥é“åŸå› 

**è§£å†³ï¼š**
```python
# æ·»åŠ è¯¦ç»†é”™è¯¯ä¿¡æ¯
assert provider.is_ollama == expected, (
    f"is_ollama should be {expected} but got {provider.is_ollama}. "
    f"api_base={provider.api_base}, model={provider.default_model}"
)
```

---

## æœ€ä½³å®è·µ

### 1. æµ‹è¯•å‘½åè§„èŒƒ

| åœºæ™¯ | å¥½çš„å‘½å | ä¸å¥½çš„å‘½å |
|-----|---------|-----------|
| æ­£å‘æµ‹è¯• | `test_is_ollama_true_with_ollama_in_model` | `test1` |
| è´Ÿå‘æµ‹è¯• | `test_is_ollama_false_without_api_base` | `test_fail` |
| è¾¹ç•Œæµ‹è¯• | `test_is_ollama_false_with_empty_api_base` | `test_edge` |

### 2. ä¸€ä¸ªæµ‹è¯•åªæµ‹ä¸€ä¸ªæ¦‚å¿µ

```python
# âœ… å¥½çš„åšæ³•ï¼šæ¯ä¸ªæµ‹è¯•ä¸€ä¸ªæ–­è¨€
def test_is_ollama_true_with_ollama_in_model(self):
    provider = create_provider("ollama/llama3.2")
    assert provider.is_ollama is True

def test_is_ollama_false_with_gpt_model(self):
    provider = create_provider("gpt-4")
    assert provider.is_ollama is False

# âŒ ä¸å¥½çš„åšæ³•ï¼šä¸€ä¸ªæµ‹è¯•å¤šä¸ªä¸ç›¸å…³æ–­è¨€
def test_is_ollama(self):  # ä¸è¦è¿™æ ·
    provider1 = create_provider("ollama/llama3.2")
    assert provider1.is_ollama is True
    
    provider2 = create_provider("gpt-4")
    assert provider2.is_ollama is False
```

### 3. ä½¿ç”¨å‚æ•°åŒ–æµ‹è¯•æ‰¹é‡éªŒè¯

```python
import pytest

@pytest.mark.parametrize("model,expected", [
    ("ollama/llama3.2", True),
    ("ollama/mistral", True),
    ("gpt-4", False),
])
def test_is_ollama_parametrized(self, model, expected):
    """ä½¿ç”¨ pytest çš„å‚æ•°åŒ–åŠŸèƒ½ã€‚"""
    provider = LiteLLMProvider(
        api_key="dummy",
        api_base="http://localhost:11434",
        default_model=model
    )
    assert provider.is_ollama == expected
```

### 4. æµ‹è¯•è¦†ç›–ç‡æ£€æŸ¥

```bash
# å®‰è£… coverage å·¥å…·
pip install pytest-cov

# è¿è¡Œæµ‹è¯•å¹¶ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
python -m pytest tests/test_litellm_provider.py --cov=nanobot.providers.litellm_provider --cov-report=html

# æŸ¥çœ‹æŠ¥å‘Š
# æ‰“å¼€ htmlcov/index.html
```

### 5. æŒç»­é›†æˆé…ç½®

GitHub Actions ç¤ºä¾‹ï¼ˆ`.github/workflows/test.yml`ï¼‰ï¼š

```yaml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install -e ".[dev]"
      
      - name: Run tests
        run: |
          pytest tests/ -v --cov=nanobot --cov-report=xml
      
      - name: Upload coverage
        uses: codecov/codecov-action@v3
```

---

## æ€»ç»“

| ä»»åŠ¡ | å‘½ä»¤ |
|-----|------|
| å®‰è£…æµ‹è¯•ä¾èµ– | `pip install -e ".[dev]"` |
| è¿è¡Œæ‰€æœ‰æµ‹è¯• | `python -m pytest -v` |
| è¿è¡Œå•ä¸ªæµ‹è¯•ç±» | `python -m pytest tests/test_litellm_provider.py::TestIsOllama -v` |
| è¿è¡Œå•ä¸ªæµ‹è¯•æ–¹æ³• | `python -m pytest tests/...::TestIsOllama::test_is_ollama_true... -v` |
| è°ƒè¯•æµ‹è¯• | VS Code + F5 æˆ– `pytest --pdb` |
| æ£€æŸ¥è¦†ç›–ç‡ | `pytest --cov=nanobot --cov-report=html` |

---

**å‚è€ƒæ–‡ä»¶ï¼š**
- æµ‹è¯•æ–‡ä»¶ï¼š`tests/test_litellm_provider.py`
- è¢«æµ‹ä»£ç ï¼š`nanobot/providers/litellm_provider.py`
- é…ç½®æ–‡ä»¶ï¼š`pyproject.toml`
